# Improving Factuality and Reasoning in Language Models through Multiagent Debate（通过多智能体辩论提高语言模型的真实性和推理能力）
太长不看版
本篇论文的目的是提高大语言模型的真实性和推理能力。作者让多个语言模型就同一问题先各自得出答案，再让这些语言模型读取并批判其他语言模型的答案来更新自己的答案，再多次重复这一辩论过程，最终得到一个收敛的答案。实验结果表明，这一方法能显著提高语言模型在许多不同类型下的答案的准确率。这一发现对提高语言模型的推理能力与事实性提供了一种新的实现方法，也能使语言模型生成的文本更具可靠性。



## 摘要
近些年来大语言模型在语言生成、语言理解、少样本学习中展现出了非凡的能力。大量工作研究如何通过提示词进一步提高模型的表现。在本文中，作者提出一种补充方法，让多个语言模型在多轮中提出自己的答案与推理过程，并与其他模型进行辩论，以此来得出共同的最终的答案。研究结果证明，这种方法能显著增强许多任务中的数学及策略推理，且增强内容的真实与有效性，减少现在模型容易出现的错误答案和幻觉。此方法还可以直接使用在现有黑箱模型，且能对所研究的所有任务使用相同的程序和提示。总的来说，这种方法有潜力显著提高大语言模型的能力，并能助力语言生成和语言理解的进一步突破。

## 1、引言
近些年来大语言模型通过在互联网上的大量文本语料库训练，但是互联网上提取的自然语言的质量和准确性无法得到保证，因此模型在经过训练后可能自信地产生错误的事实，或者在推理链上做出不可思议的跳跃。最近的大量工作集中于提高语言模型的事实准确和推理能力上。而作者注意到这些工作都是在单个语言模型上进行的，于是，他们受启发利用多个单模型分别先得到各自的结果，再让这些模型读取并批评其他模型的结果，更新自己的结果，然后重复几轮这个过程，最终得到准确率更高的答案。
![准确率比较](vx_images/221283523259287.png)

同时作者也在实验中发现辩论过后总体基本上收敛于一个更准确的答案，且这个答案中不太可能包括错误的事实（也就是模型拟造的事实），这是因为不同的模型对不确定的事实会产生分歧，所以在后续的辩论中他们会把这个模糊的事实省略。就算所有的模型在刚开始都给出错误答案，最终的讨论结果也有可能是正确的。

多模型辩论的方法不需要模型的内部信息，只需要利用模型本身，对所有的任务也只需要同一套步骤和提示。

## 2、多主体辩论中的语言生成
### 2.1多代理语言生成

作者先将同一个问题分别抛给不同的语言模型，让他们各自先得到第一轮的结果，然后利用提示词将其他模型的答案作为第二轮问题的附加题干再问这些模型一次，让他们得到第二轮的结果，重复这个过程，语言模型就会对其他模型的答案进行批判和借鉴，以此完善自己答案。
![提示](vx_images/272521800255843.png)

### 2.2辩论中的共识
根据作者的实践经验，语言模型在经过多轮辩论后能够收敛到同一个答案。
作者发现可以通过改变不同的提示词来改变语言模型在其他模型的答案下对自身原先输出结果的信任程度，来控制辩论的时间。如果模型对自己原先输出结果更加相信，辩论的持续时间就会变长。

## 3、实验
### 3.1利用多智能体辩论提高推理能力
作者利用四种不同的方法（单个模型单次作答、单个模型多次反思自己的答案、多个模型投票得出答案、多个模型多次辩论得出答案）对三种类型的问题（算术题、带有情景的小学数学题、国际象棋移动预测）进行做答，同时控制若有多轮次就为三轮次、若有多模型就用三个模型，且使用相同的提示方法。
![结果](vx_images/132410714251597.png)

反思和多模型都能提高准确率，将两者结合的方法多智能体辩论得出的答案正确率更高。有可能最初所有的模型给出的答案都是错误的，但经过辩论后得到了正确答案，所以这一方法的目的不仅是在于放大正确答案，更是在于在辩论中得出正确答案。

![](vx_images/9331714269477.png)
并且这一推理方法也能与其他推理方法相结合，在GSM8K数据集上的实验中可以看出，无论是在无思想链还是有思想链的情况下，多智能体辩论都能提高准确率。

### 3.2从多智能体辩论中提取事实信息
作者利用三种不同的方法（单个模型单次作答、单个模型多次反思自己的答案、多个模型多次辩论得出答案）对三种类型的问题（生成人物传记、对事实知识的回答、国际象棋动作的有效性判断）进行作答，控制变量与3.1相同。

![结果](vx_images/261134814267081.png)

反思无法提高答案的正确率，但多模型辩论能得到更好的结果。这是由于对问题不确定时不同的模型会给出不同的答案，如果是单模型会对自己的答案有更高的信心，但多模型进行辩论时则会把不确定的答案省略，而留下一致的答案。有意思的是，如果同一模型的许多实例都得到了相同的答案，那么就很难说服这个模型改变自己的答案，这表明说服的难易程度是评估事实可信度的一种方法。

### 3.3分析：理解多主体辩论
作者通过控制变量法，对不同因素对模型准确度的影响进行了分析，得出以下结论：

1、模型的个数及辩论的轮数。得出模型越多、辩论轮数越多，准确率就越高的结论。（四轮以上的辩论最终的结果与四轮辩论差不多）

2、辩论长度。较长提示的辩论会让答案收敛的时间更慢，但答案的共识度更高。

3、不同初始化提示。在MMLU数据集上模拟不同身份人物的语气提问（教授、医生、数学家等），发现在MMLU上模型的性能有得到提高。

4、总结。在实验的过程中作者直接将不同模型的答案加上提示词后直接给其他模型进行辩论，这样的作法是昂贵的。所以可以考虑将第一轮所有模型给出的答案先总结一下再将这个总结出来的结果喂给所有的模型进行辩论来进行辩论，发现这样做能提高辩论的性能。

5、不同的语言模型。分别利用Bard语言模型、chatGPT语言模型和两者的联合模型进行实验，发现二者的联合模型得出的答案准确率更高。

## 4、相关工作
该方法对语言模型中的推理与事实性提供了一种新的实现方法，也对语言模型生成文本的可靠性做出了贡献。

## 5、限制与讨论
本文中作者提出了多智能体辩论来提高语言模型性能的方法，发现这一方法在一系列语言建模任务中简单有效，但它也存在一定的局限性。

这一方法较其他提升方法来说更为昂贵。但作者认为也可以将它视作一种生成额外数据的方法，这些数据也可以用来提升模型性能。

此外，随着辩论轮次的增加，这一方法中模型只关注最近几轮次的输入，难以处理整个辩论过程的输入。作者认为这一问题能通过对问答上下文的改进或者用概括的方法将前面的辩论喂给模型得到缓解。

最后，虽然辩论最终会收敛到一个答案上，但这个答案不一定是正确的，而语言模型则会自信的认为这是正确的。作者认为这一问题一定程度上是因为大语言模型在生成答案的时候没有正确的表达出答案的不确定性，改善这一问题就能让改善多智能体辩论的结果。