# **Self-collaboration Code Generation via ChatGPT**

# **Self-collaboration Code Generation via ChatGPT**（通过 ChatGPT 生成自协作代码）（2024.01.19）

[原文]：[[PDF\] Self-collaboration Code Generation via ChatGPT | Semantic Scholar](https://www.semanticscholar.org/paper/Self-collaboration-Code-Generation-via-ChatGPT-Dong-Jiang/ba2f935d2578fbf77ec1aa79e26e3db396771e38)

## 精读版

### 摘要

这篇论文提出了一种新的代码生成框架，它借鉴了人类在软件开发中的团队协作方式。在这个框架中，多个大型语言模型（LLMs）扮演不同的“专家”角色，每个角色负责复杂任务中的一个特定子任务。具体来说，它设定了三个ChatGPT的角色：分析员、编码员和测试员，分别负责软件开发的分析、编码和测试阶段。这些角色通过相互协作，形成一个虚拟团队来共同完成代码生成任务，无需人类干预。实验结果表明，这种自我协作的代码生成方法在各种代码生成基准测试中表现优异，性能比直接代码生成提高了29.9%-47.1%，甚至超过了GPT-4。此外，研究还展示了这种方法在处理现实世界中的复杂任务上的潜力。



### 1 SDM启发

​        SDM 提供了一套定义明确的阶段、原则和实践，有助于有效地组织和管理团队，最终控制开发的复杂性。可有效地组织和管理团队，最终控制开发复杂性并提高软件质量。按照 SDM，我们实例化一个由三个角色（即分析员、编码员和测试员）组成的基本团队。这些角色按照工作流程，分析、编码和测试阶段依次进行，每个阶段都会对前一个阶段提供反馈。具体来说，分析员要分解需求并制定高层次计划，为编码员提供指导。然后，编码员根据计划或测试员的反馈创建或改进代码。与此同时，测试员根据编码员的结果编制测试报告，并将任何问题记录在案。我们使用三个 ChatGPT 代理通过角色指令分别扮演三个角色，然后他们在自我指导下合作完成代码生成任务。

![](Figures\snipaste20240117_105652.jpg)

**主要贡献归纳：**

​        (1) 提出了一个带有角色指示的自我协作框架，它允许 LLM 相互协作，为复杂的需求生成代码。
​        (2) 根据 SDM，实例化了一个基本团队、该团队仅由三个 ChatGPT 角色（即分析员、编码员和测试员）组成，分别负责软件开发过程中的各个阶段。在软件开发过程中负责各自的阶段。
​        (3) 在自我协作框架的基础上，由 ChatGPT（GPT-3.5）组成的虚拟团队可以在多个代码生成基准性能，甚至超过了 GPT-4。

​        (4) 在一些选定的现实世界场景中、自协作代码生成在复杂的代码生成任务中表现出显著效果。

### 2 自我协作框架

​        我们的自我协作框架由两部分组成：分工（DOL）和协作（如上图），给定一个需求 x，我们建议与 LLM 进行自我协作，生成输出 y，$\tau : x →y$

####         2.1分工部分

​           在DOL部分，论文利用先前的知识将解决复杂任务的过程分解为一系列阶段:


$$
T ⇒ [{Si}]^l_(i=1)
$$
​        基于这些阶段构建了一些不同的角色${Rj}$。每个阶段Si可以由一个或多个角色$R′j$处理。由于LLMs对上下文敏感，因此常常使用指令或提示来控制LLM生成。为了实现分工，论文引入了一种特定类型的指令，称为角色指令，用于分配角色和责任给LLMs。通过角色扮演，可以有效地将LLMs置于特定领域，并激发其在该领域内的专业知识。实证表明，与直接让LLM参与任务相比，这种角色扮演方法产生了更好的结果。需要注意的是，角色指令只需要在每个LLM代理**初始化**时提供一次，后续交互中角色将自动相互作用并传递信息，无需进一步的指令，从而提高了后续交互和协作的整体效率和清晰度。

####          2.2 合作部分

​       在DOL部分为LLM分配角色后，随着各阶段的进展，各角色将其产出与其他角色互动。在协作部分，我们的重点是促进不同角色之间的有效互动，确保它们相互促进。角色之间的交互以自然语言（NL）的形式进行，而自然语言则由语言模型的基础部分提供支持。我们在角色指令中规定了每个角色交互的角色、信息和格式，从而使整个协作过程得到很好的控制。

​        
$$
\underset{s_t}{argmax} P (s_t|s_{[<t]},R_{m(S_t)},x)
$$
​         简单来说，描述了一个分阶段的协作模型，其中每个阶段的输出都是基于**前面阶段的结果**和**当前阶段的特定角色的协作**产生的。  其中，每个阶段（表示为 $S_t$）都依赖于之前阶段的输出（表示为 $s_{[<t]}$）。在这个过程中，每个阶段都与一个特定的角色（表示为 $R_{m(S_t))}$相关联。这个角色与前面所有阶段的角色合作，以生成当前阶段的输出$（s_t$）。这个合作被视为计算  $P (s_t|s_{[<t]},R_{m(S_t)},x)$ 的一部分，其中 $P$ 表示**概率模型**。$x$表示的是需求。

​        总体而言，这个表达式在寻找在给定先前阶段的输出、当前阶段的角色和需求(requirement) x 下，使得当前阶段输出 $s_t$ 的概率最大的那个 $s_t$ 值。

​        此外，输出 y 在 $S_t$ 的进展过程中不断更新

​                                                                                          $y_t=f(s_t,y_{<t})$

​        其中，$f$是一个更新函数。一旦满足结束条件，就导出最终输出$y$。为了协调不同角色之间的协作，我们建立了一个共享的黑板,每个角色通过等式交换所需的信息来完成各自的任务 。在算法1中概述了自协作框架的伪代码。

<img src="Figures\snipaste20240117_144259.jpg" style="zoom:50%;" />

​     

### 3 实例——引入瀑布模型

​        基于一个简化的瀑布模型，包含三个阶段：**分析（analysis）**、**编码（coding）**和**测试（testing）**。这个模型被用作自协作代码生成的一个实例，其工作流程遵循瀑布模型，从一个阶段流向下一个阶段，并且如果发现问题，会回到前一个阶段进行完善。

​        在这个框架中，建立了一个由分析师、编码员和测试员组成的基本团队，分别负责分析、编码和测试阶段。各个角色的任务分配如下：

1. **分析师（Analyst）**：其目标是开发高层次的计划，并指导编码员编写程序，而不是深入到实现细节。分析师会将一个需求（x）分解成几个易于解决的子任务，以便于功能单元的划分，并制定实现的主要步骤的高层次计划。
2. **编码员（Coder）**：作为团队的核心角色，编码员接收分析师的计划或测试员的测试报告。编码员的责任包括：根据分析师提供的计划编写满足特定需求的代码，以及根据测试员反馈的测试报告来修复或完善代码。
3. **测试员（Tester）**：测试员获取编码员编写的代码，并撰写包含功能性、可读性和可维护性等方面的测试报告。该模型提倡使用模拟测试过程的方式生成测试报告，以促进交互并避免外部工作量。

​        此外，该框架还定制了适用于大型语言模型（如 ChatGPT）来扮演这三个角色的**角色指令**。举例来说，编码员的角色指令不仅包括角色描述和责任，还包括团队描述和用户需求，这些共同作用于初始化 ChatGPT 代理，设定其行为方式。角色之间也存在交互，尤其是两个连续阶段的负责角色之间，并且限制最大交互次数为 $n$。输出 $y_t$ 只在编码阶段 $S_t$ 更新，当达到 $n$ 次交互或测试员确认 $y_t$ 没有问题时，工作流程终止。

​        **role instructions** = team description + user requirement + role description

 

### 4 实验部分

​     使用了四个代码生成基准测试来展示作者提出的自协作方法的有效性。这些基准测试包括：

1. **MBPP (sanitized version)**：包含427个手动验证的Python编程任务，涵盖编程基础、标准库功能等。每个任务包含一个自然语言描述、代码解决方案和3个自动化测试用例。
2. **HumanEval**：包含164个手写编程任务。每个任务包括一个函数签名、自然语言描述、函数体和若干单元测试（平均每个任务7.7个）。
3. **MBPP-ET 和 HumanEval-ET**： MBPP 和 HumanEval 的扩展版本，每个任务增加了超过100个测试用例。这个更新版本包括边缘测试用例，与原始基准相比提高了代码评估的可靠性。

​     研究中还描述了两种代码生成的设置和基线：

- 第一种设置包括NL、函数签名和公共测试用例，用于与其他代码生成方法LLMs进行比较。
- 第二种设置仅使用NL作为输入提示，更符合现实世界的开发场景。

​       研究比较了自协作方法与多个定制和通用的大型语言模型（如 AlphaCode, Incoder, CodeGeeX, CodeGen, CodeX, ChatGPT, GPT-4）的性能。

​         所有实验中，通过其 API 调用了 ChatGPT (GPT-3.5) 的 '0301' 版本（称为 gpt-3.5-turbo），以减少模型变化对结果的意外影响。为了提高大型语言模型输出的稳定性，将解码温度设置为0。除非另有说明，研究使用仅限NL的设置进行代码生成，LLMs之间的最大交互次数限制为4。

​         通过执行测试用例来衡量 top-k 生成代码的功能正确性，可表述为
$$
Pass@k =\underset{Problems}{ \mathbb{E}}[1-\frac{\begin{pmatrix}

 n-c\\k

\end{pmatrix}}{\begin{pmatrix}
 n\\k

\end{pmatrix}} ]
$$
​        一种评估代码生成模型性能的方法，称为“Pass@k”。这个指标用于衡量在给定的测试用例下，**模型生成的前 k 个代码的功能正确性**。具体来说，Pass@k 是通过执行测试用例来评估生成的代码是否正确。
​        这个公式用于计算在给定数量的问题（或测试用例）中，模型生成的代码通过至少一个测试用例的概率。这里的 Pass@*k* 表示当模型生成前 k 个解决方案时，至少有一个解决方案通过所有测试的概率的期望值。

公式的组成部分是：

- *n*：每个问题的测试用例总数。
- *c*：通过的测试用例数量。
- *k*：考虑的代码解决方案的数量。
- 它们是组合数，分别表示从 n 个中选 k 个的方式数，以及从未通过的测试用例中选择 k 个的方式数。

公式的计算步骤是：

1. 组合数计算：表示在未通过的测试用例中随机选择 k 个的概率。
2. 然后计算 1−组合数，这表示至少有一个解决方案通过所有测试的概率。
3. 最后，计算所有问题的平均值。

​    文章中特别提到 Pass@1，这意味着主要关注的是第一个生成的代码解决方案的正确性，其实也就是贪心策略。

### 5 实验结果分析

#### 5.1 探究不同因素的提升效果

1. **自协作框架的性能提升**：研究表明，该自协作框架显著提高了基础大型语言模型（LLM）的性能。特别是，包括分析师、编码员和测试员的简单三人团队基于 ChatGPT (GPT-3.5) 的自协作代码生成，在四个代码生成基准测试中都取得了最佳性能，甚至超过了 GPT-4。与仅使用 ChatGPT (GPT-3.5) 相比，该框架提供的改进相当显著，相对增幅在 29.9% 到 34.6% 之间。

   <img src="Figures\snipaste20240117_154246.jpg" style="zoom:50%;" />

   同时，自协作代码生成在与扩展测试用例相关的数据集（如 HumanEval-ET 和 MBPP-ET）上取得了更显著的改进，表明该框架能有效协助基础 LLM 生成更可靠的代码。

2. **不同角色的性能评估**：研究还评估了在仅依赖自然语言描述（NL）的设置中，每个 ChatGPT 角色在自协作框架中的性能。结果显示，与仅使用编码员角色相比，组建团队后（无论是两角色还是三角色团队）性能都有显著提升。特别是在 HumanEval 和 HumanEval-ET 基准上，编码员-分析师-测试员团队取得了最好的结果。

   <img src="Figures\snipaste20240117_154258.jpg" style="zoom:50%;" />

3. **角色扮演策略的有效性验证**：为了进一步验证角色扮演策略的有效性，研究进行了与两个无角色扮演基线（零次射击指令和少量例子提示）的比较分析。结果表明，角色扮演方法显著优于无角色扮演的基线。

   <img src="Figures\snipaste20240117_154326.jpg" style="zoom:50%;" />

4. **互动对自协作代码生成的影响**：研究还评估了互动在自协作代码生成中的影响。实验结果显示，MI（互动次数）值从0增加到1时性能提升最大。这表明在这些基准测试中，大部分任务在两轮（即一轮互动）内就得到解决。MI值的进一步增加虽然带来的性能提升逐渐减少，但仍然观察到持续的改善。

<img src="Figures\snipaste20240117_154338.jpg" style="zoom:50%;" />

#### 5.2两个case 

​      针对自协作代码生成方法的两个案例研究，以定性评估该方法的性能。

1. **HumanEval 基准上的案例研究**：在 HumanEval 基准测试上评估了自协作代码生成。案例研究展示了该方法的工作流程和性能。首先，分析师对需求进行全面分析，并制定解决整套需求的计划。然后，编码员根据分析师提出的分解和高层次计划实现代码。测试员为实现的代码编写详细的测试报告，并识别代码中的错误，如建议去除代码中的 "lst = list(set(lst))" 行。最后，编码员根据测试报告的反馈对代码进行了修改，并由测试员确认修改后的代码无误，所有测试都通过，从而完成了代码生成过程。
2. **复杂任务的案例研究**：应用自协作代码生成方法来处理一个复杂的游戏开发需求。自协作方法生成了一个满足所有需求的游戏，无需人为干预。首先，方法正确实现了游戏逻辑，包括鼠标点击开始游戏，通过障碍物，避开炸弹，到达终点等。其次，严格遵守了需求中的规格说明，包括开始和结束点的视觉表现，游戏资源的加载，以及图像的适当缩放。此外，它还注意到了一些需求中没有提到但符合常识的游戏逻辑，例如“炸弹从屏幕顶部掉落，触底后重置位置”。相比之下，直接生成的结果只是一个Python脚本的草稿，没有包含需求中要求的所有功能。即使手动输入指令“继续添加功能”，ChatGPT 仍无法令人满意地完成这个需求。

​        这两个案例研究展示了自协作代码生成方法在处理简单功能级代码生成和更复杂的实际需求（如游戏开发）方面的有效性和优势。

