# AGENTBENCH：evaluating LLMs as agents

> 太长不看版
> 
> 这篇论文提出了一个评估大型语言模型（LLM）的代理能力的基准，即AGENTBENCH。作者在8个不同的环境下对27个基于API和开源（OSS）的大语言模型进行测试，结果表明，开源模型的性能明显落后于基于API的模型。造成一些模型表现不好的主要原因是长期推理、决策和指令跟随能力差。改进方法是通过代码调优和在高质量的多轮对齐数据上进行训练，提高代理的性能。

## **摘要**

大型语言模型（LLMs）针对超越传统NLP任务的现实世界的实用任务，正变得越来越聪明和自主。因此，迫切需要评估LLM在互动环境中完成挑战性任务的能力。作者提出了AGENTBENCH，这是一个多维的基准，由8个不同的环境组成，用于评估LLM-as-Agent在多轮开放式生成环境中的推理和决策能力。作者对27个基于API和开源（OSS）的LLM进行了广泛的测试，结果表明，顶级的商业LLM在复杂环境中呈现出强大的代理能力，与开放源码软件的竞争对手之间存在着明显的性能差异。作者确定了在特定环境下LLM表现不好的典型原因，表明长期推理、决策和指令跟随能力差是开发更好用的LLM的主要障碍。在代码调优和高质量的多轮对齐数据上进行训练可以提高代理的性能。数据集、环境和AGENTBENCH的综合评估包已经发布在https://github.com/THUDM/AgentBench.

## 1  **引言**

能够在特定环境中进行决策和行动执行的智能代理和自主实体，是人工智能的关键概念。目前大语言模型（LLMs）展示了理解人类意图和执行指令的强大能力，取得了实质性进展，但缺乏一个系统标准的基准来评估LLM-as-agent。已作出的尝试有以下缺陷：经常受到封闭的、离散的行动空间的限制，对模型基础的局限性关注，无法准确反映LLM的实际使用情况，不便于紧急评估现有的纯文本LLM，集中在单一环境中等。

为了应对这些挑战，作者提出了AGENTBENCH，旨在评估LLM-as-Agent在不同环境中的表现。AGENTBENCH包括八个不同的环境，可以分为三种基本类型：

​                ● 代码：操作系统，数据库，知识图谱

​                ● 游戏：数字纸牌游戏，横向思考谜题，家务 

​                ● 网络：网络购物，网络浏览

开发所用的数据集，无论是新创建的还是从现有的数据集改编的，都被精心设计和重新制定，以模拟交互式环境，其中纯文本的LLM可以作为自主代理运行。

作者对27个不同的LLM在AGENTBENCH上的LLM-as-Agent能力进行全面的基准测试，发现了领先的基于API的商业LLM和OSS模型之间的显著性能差距。作者揭示了现有LLM中长期推理、决策和指令遵循能力不足的问题。不同LLM之间的比较表明，改进代码训练的策略和在高质量的对齐数据上进行训练，可以帮助改善LLM-as-Agent。

为促进评估，作者还引入了一个基于服务器-客户端架构的综合工具包，拥有模块化和可扩展的特点。这使得任何使用HTTP协议的LLM都能轻松定制模型评估。这个工具包作为其相关数据集和环境的补充，现在可以被更广泛的研究界公开使用。

## 2  **定义和预备**

定义：LLM-as-Agent的交互式评估。LLM-as-Agent的交互式评估可以被看作是一个部分可观察的马尔可夫决策过程（S, A, T , R, U, O），它包括状态空间S、行动空间A、过渡函数T : S ×A → S、奖励分配函数R、任务指令空间U和观察空间O。这里，把一个LLM代理表示为M.

思维链（CoT）和其他推理策略：作为智能代理，LLMs需要强大的推理能力，AGENTBENCH中采用CoT来评估。它是最容易、最便宜和最常见的方式。

结束原因的典型类型：作者将LLM代理结束AGENTBENCH任务的原因分为五个类型。

​                ● 超过语境限制（CLE）：交互历史的长度超过了LLM的最大语境长度。

​                ● 无效格式（IF）：代理不遵循格式指令。

​                ● 无效行动（IA）：代理遵循格式指令，但其选择的行动是无效的。

​                ● 超额任务限制（TLE）：代理在达到预定的最大互动回合后不解决问题，或开始多次重复生成。

​                ● 完成（任务正常结束）。

IF和IA主要是由LLMs不良的指令跟随造成的，TLE往往表明在某些任务中多轮能力较弱。

## 3  AGENTBENCH的组成与总览

作者将AGENTBENCH的评估环境分为三个大类（ code-grounded, game-grounded,web-grounded）下的八种环境（OS,DB,KG,DCG,LTP,HH,WS,WB)，并于本段落中分别介绍其用意与评判标准。

### 3.1   CODE-GROUNDED ENVIRONMENTS

帮助人类与计算机交互是人工智能代理的一个很实际的问题。 在此，作者介绍了三个依赖编码和推理能力的环境作为代表。

#### 操作系统(OS)

作者认为目前对LLM在可执行环境中的评估较少。本文将LLMs置于真正的交互式的bash操作系统之中，令其回答一系列有确切答案的问题或令其选择执行一系列操作来达成一个目标。作者将操作的成功率（SR）作为评判标准。

#### 数据库(DB)

作者认为目前缺少对LLMs在完整的SQL环境下操作的评估，因此，本文与真实世界中一样，在真实的SQL接口、数据库、多个表和不同类型的查询中对LLMs进行评估，将成功率作为主要的评判标准。

#### 知识图谱(KG)

作者认为在一个大型的，不可被完全观察的知识图谱环境下，LLMs需要根据不完整的信息做出决策并处理固有的不确定性。这要求一个模型具有各类技巧，包括语言理解，计划制定与工具的使用。因此，本文将知识图谱环境作为一个测试LLMs决策能力的代表性测试基础，并将答案的F1分数作为评判标准。

### 3.2    GAME-GROUNDED ENVIRONMENTS

做游戏要求强大的制定策略，遵循操作指南与推理的能力。与CODE-GROUNDED的任务相比，GAME-GROUNDED的环境中的任务不需要编码方面的专业知识，而是更完整地掌握常识和世界知识。

#### 数字纸牌游戏(DCG)

DCG（例如《炉石传说》（Hoover et al.，2020））是纯文本LLM评估的理想选择。它有利于测试模型对游戏规则、操作逻辑的理解，以及基于游戏中当前条件和过去经验形成战略决策的能力。

在AGENTBENCH中，作者引入了一个简化的DCG系统Aquawar，在这个系统中，被评测的模型代理将与由作者编写的ad-hoc基线代理控制的队伍以回合制进行对战，作者将胜率(win rate)作为评判标准。

#### 横向思考谜题(LTP)

LTP指的是从非常规的角度推断事实和探索新思想的能力。数据集中，作者首先开发了一个LTP主机系统来进行自动判断。为了评估LLM的横向推理能力，作者从不同难度的网络中整理了一个多样化的谜题数据集。作者将真实情节分解为几个要点，并测量在代理耗尽最大游戏回合数时的猜测要点部分（即游戏进度），以此作为评估指标。通过这种评估，作者的目标是深入了解LLM的横向推理能力的深度和灵活性。

#### 家务 (HH, ALFWorld (Shridhar et al., 2020b))

在AGENTBENCH中，作者评估模型在经典的ALFWorld (Shridhar et al., 2020b) 的物理家务环境中完成任务的能力，ALFWorld是从公认的文本游戏工具包TextWorld (Côté et al., 2019) 衍生出来的。作者将LLM完成家务的成功率作为评判标准。

### 3.3    WEB-GROUNDED ENVIRONMENTS

此处，作者改编了两个现有的网页浏览数据集，用于对LLM进行实际评估。

#### 网上购物 (WS, WebShop (Yao et al., 2022))

网上购物的过程包括在真实的电子商务网站上搜索、查看和选择理想的商品，需要自动代理具有强大的推理和决策能力。作者采用了Mind2Web (Deng et al., 2023) 这一个模拟的网上购物环境，这个环境正好为评估语言代理这一目的服务，作者在此建议仅使用少量的提示来评估LLM。

#### 网页浏览 (WB, Mind2Web (Deng et al., 2023))

一般的网页环境是训练和评估智能代理的理想沙盒。作者在AGENTBENCH中使用了经过调整的Mind2Web (Deng et al., 2023)这一基准，在其基础上进行了调整使其无需微调可以对经过提示的LLM进行评估。

## 4  AGENTBENCH的评估

作者评估了27个现有的LLM，设计并发布了一个简单的即插即用的评估工具。

### 4.1 评估设置

#### 数据集设置

作者将每个数据集划分为Dev和Test两个部分。Dev公开且Test保密。Dev大小为269，Test大小为1091。

#### 被评估的LLMs

• 基于API的商业LLM

• 开源（OSS）LLM

作者在表一中列出了各个模型的类型，大小，创立者等信息。

![image-20240117175428128](https://github.com/linjh1118/LLM-Research/blob/main/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%80%E7%BB%84/AGENTBENCH%EF%BC%9Aevaluating%20LLMS%20as%20agents/Figures/image-20240117175428128.png)

#### 工具包

通过以API为中心的方法和环境隔离简化LLM评估。

作者开发了一个与API进行交互的评估工具包，需要评估的人员只需设置一个通过HTTP协议可访问的模型服务器。

为避免配置环境的冲突，首先，作者将具有复杂环境的任务封装成Docker镜像。研究人员可以通过挂载代码路径并轻松地启动评估过程来无障碍地使用这些镜像。其次，作者将每个任务细分为单独的工作器，确保这些任务的环境保持隔离和无冲突。

#### 评估提示设置

为适应现有的对话模型，作者围绕用户和代理两个角色构建了对话范式，并以指令和环境反馈与智能体交流。作者在此定义对话历史为( u<sub>0</sub>, a<sub>0</sub>, · · · , u<sub>k</sub>, a<sub>k</sub>)，向多轮聊天格式的最终输入设置为(u<sub>0</sub>, a<sub>r</sub>, u<sub>r+1</sub>, · · · , u<sub>k</sub>) ，r取使每个的tokens&#178;数值小于3500的最小r，并于u<sub>0</sub>附加“[NOTICE] 2r messages are omitted.”同时，作者考虑到非对话格式的模型（例如，text-davinci-003），在对话历史中每一项前面加上 “USER:”或“AGENT:”，并在最后加上字符串“AGENT:”。同时在提示的组织上在单一轮次中同时包含了“Thought”（用于CoT）和“Action”，并将推理设置解码模式为贪婪算法。

#### 总分计算

作者采用了每个任务的倒数平均分作为总分计算的固定权重，以此使低分的影响高于高分。

### 4.2   主要结果

见表3

![image-20240117193447913](https://github.com/linjh1118/LLM-Research/blob/main/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%80%E7%BB%84/AGENTBENCH%EF%BC%9Aevaluating%20LLMS%20as%20agents/Figures/image-20240117193447913.png)

结果证伪了最近一些声称OSS LLM可以与gpt-3.5-turbo和gpt-4相媲美的说法。

### 4.3   分析

不同类型的执行结果占比见表4

![image-20240117193743262](https://github.com/linjh1118/LLM-Research/blob/main/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%80%E7%BB%84/AGENTBENCH%EF%BC%9Aevaluating%20LLMS%20as%20agents/Figures/image-20240117193743262.png)

大量的任务限制超出表明大多数LLM智能体推理和决策能力薄弱。

代码训练的影响。作者发现代码调优可能深刻地影响模型的推理生成和思维方式，甚至超过代码规模对模型的影响。从codellama和llama-2系列的比较来看，用代码进行调优似乎可以使模型在遵循相对静态过程的任务中(例如，网络购物)具有优势。但是，这种调优也可能影响模型的总体思维能力，因为codellama系列在数字卡牌游戏中的表现不如llama-2系列。这指出了在调优llms时，要在善于遵循程序和善于总体思考之间取得平衡。

高质量对齐数据训练的影响。结果显示，vicuna-13b在AGENTBENCH上优于llama-2-13b，甚至与3倍大的codellama-34b表现相当。这表明高质量的对齐数据是开发更好的LLM智能体的关键。

同时，尽管llama-2-13b和llama-2-70b之间存在显著的大小差距，但它们的表现却相似。作者认为这表明了llama-2-70b的预训练不足。

## 5  相关工作

作者在此章节中列举了本篇论文的引用著作。

## 6  结论

本文首次包含了多达8个真实世界的挑战来评估LLM智能体，并建立了一个统一的测试框架和工具包用于灵活的评估。

作者认为，商业模型已经展示了作为智能体在分析、规划、执行计划、工具调用和自我反思方面的初步能力。而开源模型可能缺乏其中一些能力，或者最多只同时拥有其中的一部分。

作者期望AGENTBENCH能够成为后续研究的基石，以开发更好、更适用的智能LLM智能体。
