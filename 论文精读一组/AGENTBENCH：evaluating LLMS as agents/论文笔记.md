# AGENTBENCH：evaluating LLMs as agents

> 太长不看版
> 
> 这篇论文提出了一个评估大型语言模型（LLM）的代理能力的基准，即AGENTBENCH。作者在8个不同的环境下对27个基于API和开源（OSS）的大语言模型进行测试，结果表明，开源模型的性能明显落后于基于API的模型。造成一些模型表现不好的主要原因是长期推理、决策和指令跟随能力差。改进方法是通过代码调优和在高质量的多轮对齐数据上进行训练，提高代理的性能。

## **摘要**

大型语言模型（LLMs）针对超越传统NLP任务的现实世界的实用任务，正变得越来越聪明和自主。因此，迫切需要评估LLM在互动环境中完成挑战性任务的能力。作者提出了AGENTBENCH，这是一个多维的基准，由8个不同的环境组成，用于评估LLM-as-Agent在多轮开放式生成环境中的推理和决策能力。作者对27个基于API和开源（OSS）的LLM进行了广泛的测试，结果表明，顶级的商业LLM在复杂环境中呈现出强大的代理能力，与开放源码软件的竞争对手之间存在着明显的性能差异。作者确定了在特定环境下LLM表现不好的典型原因，表明长期推理、决策和指令跟随能力差是开发更好用的LLM的主要障碍。在代码调优和高质量的多轮对齐数据上进行训练可以提高代理的性能。数据集、环境和AGENTBENCH的综合评估包已经发布在https://github.com/THUDM/AgentBench.

![屏幕截图 2024-01-21 204133](./Figures/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-21%20204133.png)

## 1  **引言**

能够在特定环境中进行决策和行动执行的智能代理和自主实体，是人工智能的关键概念。目前大语言模型（LLMs）展示了理解人类意图和执行指令的强大能力，取得了实质性进展，但缺乏一个系统标准的基准来评估LLM-as-agent。已作出的尝试有以下缺陷：经常受到封闭的、离散的行动空间的限制，对模型基础的局限性关注，无法准确反映LLM的实际使用情况，不便于紧急评估现有的纯文本LLM，集中在单一环境中等。

为了应对这些挑战，作者提出了AGENTBENCH，旨在评估LLM-as-Agent在不同环境中的表现。AGENTBENCH包括八个不同的环境，可以分为三种基本类型：

​                ● 代码：操作系统，数据库，知识图谱

​                ● 游戏：数字纸牌游戏，横向思考谜题，家务 

​                ● 网络：网络购物，网络浏览

![屏幕截图 2024-01-21 204244](./Figures/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-21%20204244.png)

开发所用的数据集，无论是新创建的还是从现有的数据集改编的，都被精心设计和重新制定，以模拟交互式环境，其中纯文本的LLM可以作为自主代理运行。

作者对27个不同的LLM在AGENTBENCH上的LLM-as-Agent能力进行全面的基准测试，发现了领先的基于API的商业LLM和OSS模型之间的显著性能差距。作者揭示了现有LLM中长期推理、决策和指令遵循能力不足的问题。不同LLM之间的比较表明，改进代码训练的策略和在高质量的对齐数据上进行训练，可以帮助改善LLM-as-Agent。

为促进评估，作者还引入了一个基于服务器-客户端架构的综合工具包，拥有模块化和可扩展的特点。这使得任何使用HTTP协议的LLM都能轻松定制模型评估。这个工具包作为其相关数据集和环境的补充，现在可以被更广泛的研究界公开使用。

## 2  **定义和预备**

定义：LLM-as-Agent的交互式评估。LLM-as-Agent的交互式评估可以被看作是一个部分可观察的[马尔可夫决策过程](http://t.csdnimg.cn/ZW0Vk) $(S, A, T , R, U, O)$，它包括状态空间 $S$ 、行动空间 $A$ 、过渡函数 $T : S ×A → S$ 、奖励分配函数 $R$ 、任务指令空间 $U$ 和观察空间 $O$ 。这里，把一个LLM代理表示为 $M$ .

[思维链（CoT）](https://zhuanlan.zhihu.com/p/627520670)和其他推理策略：作为智能代理，LLMs需要强大的推理能力，AGENTBENCH中采用CoT来评估。它是最容易、最便宜和最常见的方式。

结束原因的典型类型：作者将LLM代理结束AGENTBENCH任务的原因分为五个类型。

​                ● 超过语境限制（CLE）：交互历史的长度超过了LLM的最大语境长度。

​                ● 无效格式（IF）：代理不遵循格式指令。

​                ● 无效行动（IA）：代理遵循格式指令，但其选择的行动是无效的。

​                ● 超额任务限制（TLE）：代理在达到预定的最大互动回合后不解决问题，或开始多次重复生成。

​                ● 完成（任务正常结束）。

IF和IA主要是由LLMs不良的指令跟随造成的，TLE往往表明在某些任务中多轮能力较弱。

## 3  AGENTBENCH的组成与总览

作者将AGENTBENCH的评估环境分为三个大类（ code-grounded, game-grounded,web-grounded）下的八种环境（OS,DB,KG,DCG,LTP,HH,WS,WB），并于本段落中分别介绍其用意与评判标准。

![屏幕截图 2024-01-21 203616](./Figures/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-21%20203616.png)

### 3.1   CODE-GROUNDED ENVIRONMENTS

帮助人类与计算机交互是人工智能代理的一个很实际的问题。 在此，作者介绍了三个依赖编码和推理能力的环境作为代表。

#### 操作系统(OS)

作者认为目前对LLM在可执行环境中的评估较少。本文将LLMs置于真正的交互式的bash操作系统之中，令其回答一系列有确切答案的问题或令其选择执行一系列操作来达成一个目标。作者将操作的成功率（SR）作为评判标准。

#### 数据库(DB)

作者认为目前缺少对LLMs在完整的SQL环境下操作的评估，因此，本文与真实世界中一样，在真实的SQL接口、数据库、多个表和不同类型的查询中对LLMs进行评估，将成功率作为主要的评判标准。

#### 知识图谱(KG)

作者认为在一个大型的，不可被完全观察的知识图谱环境下，LLMs需要根据不完整的信息做出决策并处理固有的不确定性。这要求一个模型具有各类技巧，包括语言理解，计划制定与工具的使用。因此，本文将知识图谱环境作为一个测试LLMs决策能力的代表性测试基础，并将答案的F1分数作为评判标准。

### 3.2    GAME-GROUNDED ENVIRONMENTS

做游戏要求强大的制定策略，遵循操作指南与推理的能力。与CODE-GROUNDED的任务相比，GAME-GROUNDED的环境中的任务不需要编码方面的专业知识，而是更完整地掌握常识和世界知识。

#### 数字纸牌游戏(DCG)

DCG（例如《炉石传说》（Hoover et al.，2020））是纯文本LLM评估的理想选择。它有利于测试模型对游戏规则、操作逻辑的理解，以及基于游戏中当前条件和过去经验形成战略决策的能力。

在AGENTBENCH中，作者引入了一个简化的DCG系统Aquawar，在这个系统中，被评测的模型代理将与由作者编写的ad-hoc基线代理控制的队伍以回合制进行对战，作者将胜率(win rate)作为评判标准。

#### 横向思考谜题(LTP)

它的名字来自心理学术语lateral thinking(De Bono, 1970)，指的是从非常规的角度推断事实和探索新思想的能力。数据集中，作者首先开发了一个LTP主机系统来进行自动判断。为了评估LLM的横向推理能力，作者从不同难度的网络中整理了一个多样化的谜题数据集。作者将真实情节分解为几个要点，并测量在代理耗尽最大游戏回合数时的猜测要点部分（即游戏进度），以此作为评估指标。通过这种评估，作者的目标是深入了解LLM的横向推理能力的深度和灵活性。

#### 家务 (HH, ALFWorld (Shridhar et al., 2020b))

在AGENTBENCH中，作者评估模型在经典的ALFWorld (Shridhar et al., 2020b) 的物理家务环境中完成任务的能力，ALFWorld是从公认的文本游戏工具包TextWorld (Côté et al., 2019) 衍生出来的。作者将LLM完成家务的成功率作为评判标准。

### 3.3    WEB-GROUNDED ENVIRONMENTS

此处，作者改编了两个现有的网页浏览数据集，用于对LLM进行实际评估。

#### 网上购物 (WS, WebShop (Yao et al., 2022))

网上购物的过程包括在真实的电子商务网站上搜索、查看和选择理想的商品，需要自动代理具有强大的推理和决策能力。作者采用了Mind2Web (Deng et al., 2023) 这一个模拟的网上购物环境，这个环境正好为评估语言代理这一目的服务，作者在此建议仅使用少量的提示来评估LLM。

#### 网页浏览 (WB, Mind2Web (Deng et al., 2023))

一般的网页环境是训练和评估智能代理的理想沙盒。作者在AGENTBENCH中使用了经过调整的Mind2Web (Deng et al., 2023)这一基准，在其基础上进行了调整使其无需微调可以对经过提示的LLM进行评估。

## 4  AGENTBENCH的评估

作者评估了27个现有的LLM，设计并发布了一个简单的即插即用的评估工具。

### 4.1 评估设置

#### 数据集设置

作者将每个数据集划分为Dev和Test两个部分。Dev公开且Test保密。Dev大小为269，Test大小为1091。

#### 被评估的LLMs

• 基于API的商业LLM

• 开源（OSS）LLM

作者在表1中列出了各个模型的类型，大小，创立者等信息。

![image-20240117175428128](https://github.com/linjh1118/LLM-Research/blob/main/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%80%E7%BB%84/AGENTBENCH%EF%BC%9Aevaluating%20LLMS%20as%20agents/Figures/image-20240117175428128.png)

#### 工具包

![屏幕截图 2024-01-21 213502](./Figures/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-21%20213502.png)

AGENTBENCH的工具包经过精心设计，再加上一个高效的评估分配系统，实现了任务和agent的无缝部署。

代理服务器(左)以不同的形式显示，使我们能够部署模型服务器，并通过HTTP协议公开可访问的API。

任务服务器(右)由一个任务控制器和几个任务工作器组成。首先，作者将具有复杂环境的任务封装成[Docker镜像](http://t.csdnimg.cn/XETDE)。研究人员可以通过挂载代码路径并轻松地启动评估过程来无障碍地使用这些镜像。其次，作者将每个任务细分为单独的工作器，确保这些任务的环境保持隔离和无冲突。

评价客户端(中)建立代理-任务图，采用最大流算法优化交互。这种优化导致客户端工人无缝地与代理和任务服务器进行交互，促进任务和评估的顺利执行。
作者采用Edmonds-Karp算法(Edmonds & Karp, 1972)作为Ford±
Fulkerson算法(Ford Jr & Fullkerson, 1962)的实际实现，旨在计算时间复杂度为  $O\left(|V||E|^{2}\right)$ 的网络中的最大流量。

考虑一个有 $n$ 个代理的场景，表示为 $( A_{1}, A_{2}, \cdots, A_{n} )$ ，以及 $m$ 个任务，表示为 $( T_{1}, T_{2}, \cdots, T_{m} )$ . 目标是在 $l$ 个不同的组中进行评估， 每个组关注的是一对 $\left(A_{x_{k}}, T_{y_{k}}\right)$ ，其中 $1 \leq k \leq l$ . 对于每一对 $\left(A_{x_{k}}, T_{y_{k}}\right)$ , 应该评估样本 $s_{k}$ . 代理 $A_{k}$ 和任务 $T_{k}$ 的工人数分别记为 $w\left(A_{k}\right)$ 和 $w\left(T_{k}\right)$ .

由此构建的流图可以描述为 $G=\langle V, E\rangle$ , 其中顶点集合  $V$  定义为
$$ \begin{aligned}
V= & \left\{A_{k} \mid 1 \leq k \leq n\right\} \\
& \cup\left\{T_{k} \mid 1 \leq k \leq m\right\} \\
& \cup\{S, D\},
\end{aligned}$$ 
而加权边集 $E$ 记为
 $$\begin{aligned}
E= & \left\{\left(A_{x_{k}}, T_{y_{k}}, s_{k}\right) \mid 1 \leq k \leq l\right\} \\
& \cup\left\{\left(S, A_{k}, w\left(A_{k}\right) \mid 1 \leq k \leq n\right\}\right. \\
& \cup\left\{\left(T_{k}, D, w\left(T_{k}\right) \mid 1 \leq k \leq m\right\} .\right.
\end{aligned}$$ 

作者应用最大流算法从源顶点 $S$ 到目的顶点 $D$ . 对于每个流边 $\left(A_{i}, T_{j}, f_{(i, j)}\right)$ , 为代理 $A_{i}$ 和任务 $T_{j}$ 分配样本 $f_{(i, j)}$ .分配后，边的权重应该通过流量的值来减少。完成评估后，连接到 $S$ 或 $D$ 的边的权重应该增加1。
作者还建立了一个周期间隔，用于将算法应用于新可用的评估三元组的网络。

#### 评估提示设置

为适应现有的对话模型，作者围绕用户和代理两个角色构建了对话范式，并以指令和环境反馈与智能体交流。作者在此定义对话历史为 $\left(u_{0}, a_{0}, \cdots, u_{k}\right)$ ，向多轮聊天格式的最终输入设置为 $\left(u_{0}, a_{r}, u_{r+1}, \cdots, u_{k}\right)$ ， $r$ 取使每条历史的tokens计数小于3500的最小值，并于 $u$ <sub>0</sub>附加“[NOTICE]  $2r$  messages are omitted(省略).”同时，作者考虑到非对话格式的模型（例如text-davinci-003），在对话历史中每一项前面加上 “USER:”或“AGENT:”，并在最后加上字符串“AGENT:”。同时在提示的单一轮次中同时包含了“Thought”（用于CoT）和“Action”，并将推理设置解码模式为贪婪解码。

*[tokens]:可理解为词语片段，输入输出按一定规则被分割成若干tokens，来计算总长度

*[贪婪解码]:局部最优的贪心搜索方法，每一步都选择概率最大的token进行输出

#### 总分计算

作者采用了每个任务的倒数平均分作为总分计算的固定权重，以此使低分的影响高于高分。

### 4.2   主要结果

见表3

![image-20240117193447913](https://github.com/linjh1118/LLM-Research/blob/main/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%80%E7%BB%84/AGENTBENCH%EF%BC%9Aevaluating%20LLMS%20as%20agents/Figures/image-20240117193447913.png)

结果显示，大多数开源llms在AGENTBENCH中的表现远不如基于api的llms(平均0.51 vs 平均2.15)，证伪了最近一些声称OSS LLM可以与gpt-3.5-turbo和gpt-4相媲美的说法。

### 4.3   分析

不同类型的执行结果部分。任务限制超出是导致AGENTBENCH任务不完整的主要原因。这意味着大多数LLM智能体虽然遵循指令，但在给定时间内无法解决挑战，或者在交互增加时陷入重复生成，说明推理和决策能力较弱。此外，任务的预期输出可能接近某些模型的训练数据，但不精确地一致。这种差异可能导致模型恢复到它们预先训练的格式，无意中忽略了提供的特定要求。

![image-20240117193743262](https://github.com/linjh1118/LLM-Research/blob/main/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%80%E7%BB%84/AGENTBENCH%EF%BC%9Aevaluating%20LLMS%20as%20agents/Figures/image-20240117193743262.png)

代码训练的影响。但同时，作者认为代码调优可能深刻地影响模型的推理生成和思维方式，甚至超过代码规模对模型的影响。从codellama和llama-2系列的比较来看，用代码进行调优似乎可以使模型在遵循相对静态过程的任务中(例如网络购物)具有优势。然而，代码调优的缺点似乎是模型的逻辑推理能力和情景感知能力的妥协，因为codellama系列在数字卡牌游戏中的表现不如llama-2系列。这指出了在调优llms时，要在善于遵循程序和善于总体思考之间取得平衡。

![屏幕截图 2024-01-21 175908](./Figures/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-21%20175908.png)

高质量对齐数据训练的影响。vicuna-13b和llama-2-13b拥有相同的基础LLM，vicuna-13b在ShareGPT的对齐数据上训练，而llama-2-13b在原始的对齐数据上训练。结果显示，vicuna-13b优于llama-2-13b，甚至与3倍大的codellama-34b表现相当。这表明高质量的对齐数据是开发更好的LLM智能体的关键。

![屏幕截图 2024-01-21 205759](./Figures/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-01-21%20205759.png)

在许多测试案例中，模型失败的主要原因是它无法从环境提供的错误反馈中识别自己的错误。这一点在DB任务中表现得尤为明显。具有自我纠正SQL语句能力的模型明显优于其他模型。

同时，尽管llama-2-13b和llama-2-70b之间存在显著的大小差距，但它们的表现却相似。作者认为这表明了llama-2-70b的预训练不足。根据缩放定律(Hoffmann et al., 2022)，llama-2-70b的预训练应当使用更大token的数据。



## 5  相关工作

作者在此章节中列举了本篇论文的引用著作。

## 6  结论

本文首次包含了多达8个真实世界的挑战来评估LLM智能体，并建立了一个统一的测试框架和工具包用于灵活的评估。

作者认为，商业模型已经展示了作为智能体在分析、规划、执行计划、工具调用和自我反思方面的初步能力。而开源模型可能缺乏其中一些能力，或者最多只同时拥有其中的一部分。

作者期望AGENTBENCH能够成为后续研究的基石，以开发更好、更适用的智能LLM智能体。


